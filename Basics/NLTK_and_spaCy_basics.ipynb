{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One time Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "nltk.download('all')\n",
    "!pip3 install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOQflwW4IRtz"
   },
   "source": [
    "## **NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khJkXLNEQI6t",
    "outputId": "1bcf5058-db9f-4455-93c6-ac0b60a7d58a"
   },
   "outputs": [],
   "source": [
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYGwWe7P_EY"
   },
   "source": [
    "#### **Tokenization**\n",
    "Splitting up a text into meaning units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-zW1xnsbQE8t"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mQtvqhjpSWTv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rohit', 'Raj', 'is', 'the', 'new', 'Mayor', 'of', 'the', 'town', '!', '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Rohit Raj is the new Mayor of the town!!'\n",
    "tokens = word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ponvefpJr7_F"
   },
   "source": [
    "#### **Stopwords**\n",
    "\n",
    "Removing not meaningful words like is of the an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "icnrRdz3sKlX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rohit', 'Raj', 'new', 'Mayor', 'town', '!', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "sentence = 'Rohit Raj is the new Mayor of the town!!'\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "word_tokens = word_tokenize(sentence)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n",
    "  \n",
    "filtered_sentence = []  \n",
    "  \n",
    "for w in word_tokens:  \n",
    "    if w not in stop_words:  \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDMtaIZEZ2w7"
   },
   "source": [
    "#### **Stemming**\n",
    "Stemming is the process of reducing a word to its stem by cutting it off at beginnnig or end.\n",
    "\n",
    "**Note:** Not everytime stemming will give a result which will have a meaningfull meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZzRp-daKVSB5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('win', 'studi', 'buy')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "stemming.stem('Winning'),stemming.stem('studies'),stemming.stem('buys')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDUQ60NkcIWE"
   },
   "source": [
    "#### **Lemmatization**\n",
    "Lemmatization is the process of reducing a word to its lemma or dictionary form\n",
    "Note: Unlike stemming will give a meaning result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WQUIsDKJcHZC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Winning', 'study', 'buy')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "lemma.lemmatize('Winning'),lemma.lemmatize('studies'),lemma.lemmatize('buys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ4A48b80EQM"
   },
   "source": [
    "#### **Parts of Speech Tagging (POS Tagging)** \n",
    "\n",
    "Process of Marking up a word of in a text (corpus) as corresponding to ap every token according to Grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vcAyt8g-0CUs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Process', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('Marking', 'VBG')]\n",
      "[('up', 'RB')]\n",
      "[('a', 'DT')]\n",
      "[('word', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('in', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('text', 'NN')]\n",
      "[('(', '(')]\n",
      "[('corpus', 'NN')]\n",
      "[(')', ')')]\n",
      "[('as', 'IN')]\n",
      "[('corresponding', 'VBG')]\n",
      "[('to', 'TO')]\n",
      "[('ap', 'NN')]\n",
      "[('every', 'DT')]\n",
      "[('token', 'NN')]\n",
      "[('according', 'VBG')]\n",
      "[('to', 'TO')]\n",
      "[('Grammer', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = 'Process of Marking up a word of in a text (corpus) as corresponding to ap every token according to Grammer'\n",
    "\n",
    "pos_tokenize  = word_tokenize(text)\n",
    "\n",
    "# pos_tokenize\n",
    "\n",
    "for token in pos_tokenize:\n",
    "  print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxNNDNUxB8Fc"
   },
   "source": [
    "#### **Named Entity Recognition (NER)**\n",
    "Process of taking a string as a text if input and identifying relevant nouns (people places and organizations) that are mentioned in that string. NER Entities:\n",
    "\n",
    "1. Facility\n",
    "2. Person\n",
    "3. Location\n",
    "4. Organization\n",
    "5. Geo-Political Entity\n",
    "6. Geo-Social Political Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mcdwl6Ez5C57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NE Tag:  (S (PERSON John/NNP))\n",
      "NE Tag:  (S lives/NNS)\n",
      "NE Tag:  (S in/IN)\n",
      "NE Tag:  (S (GPE New/NNP))\n",
      "NE Tag:  (S (GPE York/NNP))\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "john  = ' John lives in New York'\n",
    "\n",
    "john_tokens = word_tokenize(john)\n",
    "\n",
    "for tok in john_tokens:\n",
    "  pos_tag = nltk.pos_tag([tok])\n",
    "  print(\"NE Tag: \", ne_chunk(pos_tag)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlDthdNMS_Qr"
   },
   "source": [
    "#### **N Grams**\n",
    "It is collection of N tokens at time\n",
    "\n",
    "Most commonly used Grams are :\n",
    "\n",
    "**Unigrams:** one word at a time\n",
    "\n",
    "**Bigrams:** 2 words at a time\n",
    "\n",
    " **Trigrams:** 3 words at a time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "d2p66RozSo-h"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('No', 'way'),\n",
       " ('way', 'was'),\n",
       " ('was', 'he'),\n",
       " ('he', 'going'),\n",
       " ('going', 'to'),\n",
       " ('to', 'turn'),\n",
       " ('turn', 'himself'),\n",
       " ('himself', 'in'),\n",
       " ('in', ','),\n",
       " (',', 'not'),\n",
       " ('not', 'in'),\n",
       " ('in', 'this'),\n",
       " ('this', 'life'),\n",
       " ('life', '.'),\n",
       " ('.', 'A'),\n",
       " ('A', 'werewolf'),\n",
       " ('werewolf', 'would'),\n",
       " ('would', 'not'),\n",
       " ('not', 'kill'),\n",
       " ('kill', 'himself'),\n",
       " ('himself', ','),\n",
       " (',', 'only'),\n",
       " ('only', 'frame'),\n",
       " ('frame', 'others'),\n",
       " ('others', '.'),\n",
       " ('.', 'That'),\n",
       " ('That', 'was'),\n",
       " ('was', 'the'),\n",
       " ('the', 'only'),\n",
       " ('only', 'way'),\n",
       " ('way', 'to'),\n",
       " ('to', 'live'),\n",
       " ('live', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams, ngrams\n",
    "\n",
    "novel = 'No way was he going to turn himself in, not in this life. A werewolf would not kill himself, only frame others. That was the only way to live.'\n",
    "\n",
    "novel_token = word_tokenize(novel)\n",
    "\n",
    "list(nltk.bigrams(novel_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4rjaNbvlVJ03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('No', 'way', 'was'),\n",
       " ('way', 'was', 'he'),\n",
       " ('was', 'he', 'going'),\n",
       " ('he', 'going', 'to'),\n",
       " ('going', 'to', 'turn'),\n",
       " ('to', 'turn', 'himself'),\n",
       " ('turn', 'himself', 'in'),\n",
       " ('himself', 'in', ','),\n",
       " ('in', ',', 'not'),\n",
       " (',', 'not', 'in'),\n",
       " ('not', 'in', 'this'),\n",
       " ('in', 'this', 'life'),\n",
       " ('this', 'life', '.'),\n",
       " ('life', '.', 'A'),\n",
       " ('.', 'A', 'werewolf'),\n",
       " ('A', 'werewolf', 'would'),\n",
       " ('werewolf', 'would', 'not'),\n",
       " ('would', 'not', 'kill'),\n",
       " ('not', 'kill', 'himself'),\n",
       " ('kill', 'himself', ','),\n",
       " ('himself', ',', 'only'),\n",
       " (',', 'only', 'frame'),\n",
       " ('only', 'frame', 'others'),\n",
       " ('frame', 'others', '.'),\n",
       " ('others', '.', 'That'),\n",
       " ('.', 'That', 'was'),\n",
       " ('That', 'was', 'the'),\n",
       " ('was', 'the', 'only'),\n",
       " ('the', 'only', 'way'),\n",
       " ('only', 'way', 'to'),\n",
       " ('way', 'to', 'live'),\n",
       " ('to', 'live', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(novel_token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tNd1CTnnVJ2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('No', 'way', 'was', 'he'),\n",
       " ('way', 'was', 'he', 'going'),\n",
       " ('was', 'he', 'going', 'to'),\n",
       " ('he', 'going', 'to', 'turn'),\n",
       " ('going', 'to', 'turn', 'himself'),\n",
       " ('to', 'turn', 'himself', 'in'),\n",
       " ('turn', 'himself', 'in', ','),\n",
       " ('himself', 'in', ',', 'not'),\n",
       " ('in', ',', 'not', 'in'),\n",
       " (',', 'not', 'in', 'this'),\n",
       " ('not', 'in', 'this', 'life'),\n",
       " ('in', 'this', 'life', '.'),\n",
       " ('this', 'life', '.', 'A'),\n",
       " ('life', '.', 'A', 'werewolf'),\n",
       " ('.', 'A', 'werewolf', 'would'),\n",
       " ('A', 'werewolf', 'would', 'not'),\n",
       " ('werewolf', 'would', 'not', 'kill'),\n",
       " ('would', 'not', 'kill', 'himself'),\n",
       " ('not', 'kill', 'himself', ','),\n",
       " ('kill', 'himself', ',', 'only'),\n",
       " ('himself', ',', 'only', 'frame'),\n",
       " (',', 'only', 'frame', 'others'),\n",
       " ('only', 'frame', 'others', '.'),\n",
       " ('frame', 'others', '.', 'That'),\n",
       " ('others', '.', 'That', 'was'),\n",
       " ('.', 'That', 'was', 'the'),\n",
       " ('That', 'was', 'the', 'only'),\n",
       " ('was', 'the', 'only', 'way'),\n",
       " ('the', 'only', 'way', 'to'),\n",
       " ('only', 'way', 'to', 'live'),\n",
       " ('way', 'to', 'live', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(novel_token,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxxAyR3fGwo0"
   },
   "source": [
    "#### **Finding Most Frequently occuring tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y_8zXt8kQMet"
   },
   "outputs": [],
   "source": [
    "# string\n",
    "text = \"Seeing that the outside world was having more and more doubts, with the mounting pressure from the upper echelons,Sherlock had no choice but to reluctantly pick out some of the forged evidence and form an evidence chain using his deductive skills.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WMwEkLiLQtNn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 45)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragrah_tokens = word_tokenize(text)\n",
    "\n",
    "#checking type and number of tokens \n",
    "type(paragrah_tokens), len(paragrah_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GvjGjYzrRZrg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 4, 'more': 2, 'and': 2, ',': 2, 'evidence': 2, 'Seeing': 1, 'that': 1, 'outside': 1, 'world': 1, 'was': 1, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdlist = FreqDist(paragrah_tokens)\n",
    "fdlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IEIdbZV1Rn78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4),\n",
       " ('more', 2),\n",
       " ('and', 2),\n",
       " (',', 2),\n",
       " ('evidence', 2),\n",
       " ('Seeing', 1),\n",
       " ('that', 1),\n",
       " ('outside', 1),\n",
       " ('world', 1),\n",
       " ('was', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding N topmost commonly occuring words\n",
    "top_10 = fdlist.most_common(10)\n",
    "top_10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK0NtXHiXj5m"
   },
   "source": [
    "#### **Chunking**\n",
    "\n",
    "Technique used to group words or tokens into phrases or order to analyze the structure of a sentence. This grouping includes POS tags as well as phrases from a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pVfYqmRYotp"
   },
   "source": [
    "##### Chunking Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "w9KmTtgMXrHY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('crazy', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('dog', 'NN'),\n",
       " ('went', 'VBD'),\n",
       " ('running', 'VBG'),\n",
       " ('through', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mud', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'The crazy brown dog went running through the mud.'\n",
    "\n",
    "tokens= word_tokenize(sample)\n",
    "\n",
    "pos_text = nltk.pos_tag(tokens)\n",
    "pos_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tq1B-foTXrLL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  crazy/JJ\n",
      "  brown/NN\n",
      "  dog/NN\n",
      "  went/VBD\n",
      "  running/VBG\n",
      "  through/IN\n",
      "  the/DT\n",
      "  mud/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammer = (r''' \n",
    "            NP: {<DT>?<JJ>*<NN>*}  // first word determiner,? for 0 or 1 occurence,JJ for adjective, * for 0 or any number of occurence, NN for Noun\n",
    "        ''')\n",
    "\n",
    "chunkParser = nltk.RegexpParser(grammer)\n",
    "tree = chunkParser.parse(pos_text)\n",
    "\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3rveDwYWXrWw"
   },
   "outputs": [],
   "source": [
    "# tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKJHzt05fyLw"
   },
   "source": [
    "##### Chunking Verb Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-b33yMcEf0XJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  crazy/JJ\n",
      "  brown/NN\n",
      "  dog/NN\n",
      "  went/VBD\n",
      "  running/VBG\n",
      "  through/IN\n",
      "  the/DT\n",
      "  mud/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sample = 'He should wait before going swimming'\n",
    "\n",
    "pos_tag = nltk.pos_tag(word_tokenize(sample))\n",
    "\n",
    "grammer = (r'''\n",
    "          VP: {<PRP>?<VB|VBD|VBZ|VBG>*<RB|RBR>}\n",
    "          ''')\n",
    "\n",
    "chunkParser = nltk.RegexpParser(grammer)\n",
    "\n",
    "tree = chunkParser.parse(pos_text)\n",
    "\n",
    "print(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuPqWxVwjzSN"
   },
   "source": [
    "#### **Chinking**\n",
    "\n",
    "used to exclude a specific chunk from the whole chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ReAn7pLLf0Zd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (Chunk crazy/JJ brown/NN dog/NN)\n",
      "  went/VBD\n",
      "  running/VBG\n",
      "  through/IN\n",
      "  the/DT\n",
      "  (Chunk mud/NN ./.))\n"
     ]
    }
   ],
   "source": [
    "sample = 'The crazy brown dog went running through the mud.'\n",
    "\n",
    "tokens= word_tokenize(sample)\n",
    "\n",
    "pos_text = nltk.pos_tag(tokens)\n",
    "pos_text\n",
    "\n",
    "\n",
    "grammer = (r\"\"\" Chunk:  {<.*>+}\n",
    "          }<VB.?|IN|DT|TO>+{\n",
    "          \"\"\")\n",
    "\n",
    "\n",
    "chunkParser = nltk.RegexpParser(grammer)\n",
    "tree = chunkParser.parse(pos_text)\n",
    "\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJz15cdiIZ8Z"
   },
   "source": [
    "## **spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yfvOP82IIcJM"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# You need to load spacy models before using any of the functionalities \n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17k-sxb2Sz4y"
   },
   "source": [
    "#### **Tokenization**\n",
    "Splitting up a text into meaning units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a393RN-2RPYe",
    "outputId": "a091ce50-d341-4e2b-feeb-b3202d9a1c81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0  Token: Rohit\n",
      "Index:  1  Token: Raj\n",
      "Index:  2  Token: is\n",
      "Index:  3  Token: the\n",
      "Index:  4  Token: new\n",
      "Index:  5  Token: Mayor\n",
      "Index:  6  Token: of\n",
      "Index:  7  Token: the\n",
      "Index:  8  Token: town\n",
      "Index:  9  Token: !\n",
      "Index:  10  Token: !\n"
     ]
    }
   ],
   "source": [
    "#creating document using nlp object. \n",
    "doc = nlp('Rohit Raj is the new Mayor of the town!!')\n",
    "\n",
    "for token in doc:\n",
    "  print('Index: ',token.i,' Token:',token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OArpSRYCR-CE",
    "outputId": "c1a29905-0ba1-4d0f-b6a5-2b78ce3b9c0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is the new"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[2:5]\n",
    "span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGnrlUsqTQ5F"
   },
   "source": [
    "#### **Parts of Speech Tagging (POS Tagging)** \n",
    "\n",
    "Process of Marking up a word of in a text (corpus) as corresponding to ap every token according to Grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZ1mKXo_Tg3_",
    "outputId": "b2e2d2ce-9330-44fc-bfd2-2e82e9835daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0  Token: Process  POS Tag: NOUN\n",
      "Index: 1  Token: of  POS Tag: ADP\n",
      "Index: 2  Token: Marking  POS Tag: PROPN\n",
      "Index: 3  Token: up  POS Tag: ADP\n",
      "Index: 4  Token: a  POS Tag: DET\n",
      "Index: 5  Token: word  POS Tag: NOUN\n",
      "Index: 6  Token: of  POS Tag: ADP\n",
      "Index: 7  Token: in  POS Tag: ADP\n",
      "Index: 8  Token: a  POS Tag: DET\n",
      "Index: 9  Token: text  POS Tag: NOUN\n",
      "Index: 10  Token: (  POS Tag: PUNCT\n",
      "Index: 11  Token: corpus  POS Tag: X\n",
      "Index: 12  Token: )  POS Tag: PUNCT\n",
      "Index: 13  Token: as  POS Tag: ADP\n",
      "Index: 14  Token: corresponding  POS Tag: VERB\n",
      "Index: 15  Token: to  POS Tag: PART\n",
      "Index: 16  Token: ap  POS Tag: VERB\n",
      "Index: 17  Token: every  POS Tag: DET\n",
      "Index: 18  Token: token  POS Tag: ADJ\n",
      "Index: 19  Token: according  POS Tag: VERB\n",
      "Index: 20  Token: to  POS Tag: ADP\n",
      "Index: 21  Token: Grammer  POS Tag: PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Process of Marking up a word of in a text (corpus) as corresponding to ap every token according to Grammer')\n",
    "for token in doc:\n",
    "  print('Index:',token.i,' Token:' ,token.text,' POS Tag:' ,token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SnpJL9eTV9v"
   },
   "source": [
    "#### **Named Entity Recognition (NER)**\n",
    "Process of taking a string as a text if input and identifying relevant nouns (people places and organizations) that are mentioned in that string.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsR2He6ISFIJ",
    "outputId": "568857de-3392-4e5b-c1f8-08827a8a463d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York GPE\n",
      "Barack Obama PERSON\n",
      "White House ORG\n",
      "today DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(' John lives in New York')\n",
    "\n",
    "for entity in doc.ents:\n",
    "  print(entity.text, entity.label_)\n",
    "\n",
    "\n",
    "doc = nlp('Barack Obama, former president will be vacating White House today')  \n",
    "for entity in doc.ents:\n",
    "  print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFacZwiQFE6H"
   },
   "source": [
    "#### **Matcher**\n",
    "\n",
    "Helps us to find patterns in a string with different criteria. Ex: Helping us understand if the word is lemma or punctuation or not\n",
    "\n",
    "Link: https://spacy.io/usage/rule-based-matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mk0l_LWA2iMK",
    "outputId": "b1b6c3a3-2270-46ac-ec61-0a394d789621"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4191279314630736679, 10, 12)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "doc = nlp('Barack Obama the former president of United States will be vacating white house today')  \n",
    "\n",
    "pattern = [{'LEMMA': 'vacate'}, {'ORTH': 'white'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('white_Pattern',[pattern])\n",
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifI7-38VIYAg",
    "outputId": "cbb2cb74-bee8-43b0-be27-b2fe962d64ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vacating white\n"
     ]
    }
   ],
   "source": [
    "for match_id,start,end in matches:\n",
    "  matched_span  = doc[start:end]\n",
    "  print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNsiPLglJVTk",
    "outputId": "2efbded0-e71f-481f-a075-9bca6b1b6506"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10959151708183927808, 0, 3)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('2018 FIFA World Cup: Frace won!!')\n",
    "\n",
    "pattern = [{'IS_DIGIT': True}, {'LOWER': 'fifa'}, {'LOWER': 'world'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('fifa_Pattern',[pattern])\n",
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmRoSu4LKu_T",
    "outputId": "534b1595-38e4-4ecd-e286-385de536fc1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World\n"
     ]
    }
   ],
   "source": [
    "for match_id,start,end in matches:\n",
    "  matched_span  = doc[start:end]\n",
    "  print(matched_span.text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GOQflwW4IRtz",
    "oDMtaIZEZ2w7",
    "BDUQ60NkcIWE",
    "xlDthdNMS_Qr",
    "IxxAyR3fGwo0"
   ],
   "name": "NLTK and spaCy basics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
